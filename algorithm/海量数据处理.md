## 概念

- 海量数据处理：基于海量数据的存储，处理和操作，海量数据一般难以一次性装载入内存，或者短时间难以迅速解决。

## 解决方法

### 哈希分治

- 方法介绍
  - 无法一次性将海量数据装载入内存，因此考虑将数据通过hash映射，分割成相应的小数据块，然后再针对各个数据块通过hashmap进行统计或者其他操作。
- 实例
  - 寻找访问Top IP：
    1. 分而治之：提取日志IP后，根据Hash映射将IP分到1000个小文件中，
    2. Hashmap统计：使用Hashmap统计频次
    3. 堆排序／快排：构建小顶堆进行排序求处最高频次IP，或者依据快排思想，每次排除一半数，最后找出最大数。
  - 寻找热门查询关键字：
    1. 查询记录过千万，首先去重，然后将其放入HashMap中，统计频次。
  - 寻找频数最高的100个词
    1. 将1G大小的文件，每行一个词，大小不超过16字节，内存限制为1M，返回频数最高的100个词。
    2. 分而治之或者hash映射进行分块，然后对每个小文件统计词频。
  - 寻找100台电脑上的Top10
    1. 如果同一数据元素只在某一台电脑，那么直接使用对排序分别求出100台电脑每台电脑的Top10
    2. 将100台电脑的Top10归并，最后求出整体Top10
    3. 如果数据分布在不同的机器上，那么可以先重新hash取模，使得每个元素都出现在同一台电脑，然后再按照上述方法进行求Top10
  - 寻找共同的url
    1. a和b两个文件夹，各存放50亿url，每个url占64字节，内存限制4G，找出共同url
    2. 首先对ab文件分别进行hash映射，然后存放到1000个文件中，那么可能相同的url都放到同一文件夹中了。
    3. 然后使用hashset完成这项工作